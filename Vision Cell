Volume Electron Microscopy (VEM) reconstructs the internal 3D structure of samples by sequentially removing material layers and imaging each layer, making it widely used in fields such as cell biology, neuroscience, and materials science. However, improving 3D reconstruction accuracy often requires extremely thin slices, which can damage fragile samples. Although existing deep learning methods can transform 2D images into  3D reconstructions, they generally rely on large amounts of high-quality data, the acquisition of which necessitates finer slicing, thus increasing the risk of sample damage. This creates a "chicken-and-egg" dilemma: insufficient data quality hinders model training, yet obtaining high-quality data may damage the sample, reducing reconstruction accuracy. To address this issue, we propose the Vision-Cell framework, an innovative deep learning based method capable of high-precision simulation of 2D scan images under limited data conditions. It extends to 3D and even 4D reconstructions (including cell growth processes) while minimizing physical damage to the sample. Validated through experiments on plant cells, Vision-Cell demonstrates exceptional reconstruction performance even in sparse data environments, accurately simulating dynamic changes in cells. Vision-Cell holds great potential for broader applications in neuroscience, structural biology, and materials science, offering new opportunities and breakthroughs for related research.
